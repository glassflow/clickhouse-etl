---
title: 'Helm Values Configuration'
description: 'Complete guide to configuring GlassFlow using Helm values.yaml'
---

import { Callout } from 'nextra/components'
import { Tabs } from 'nextra/components'
import { Steps } from 'nextra/components'
import { Cards } from 'nextra/components'

# GlassFlow Helm Values Configuration

This comprehensive guide covers all available configuration options in the GlassFlow Helm chart's `values.yaml` file. Use this reference to customize your GlassFlow deployment for production environments.

<Callout type="important">
**Quick Start**: For basic installations, you can use the default values. For production deployments, review the sections below to optimize your configuration.
</Callout>

## Global Settings

Global settings apply across all components of the GlassFlow deployment.

```yaml
global:
  # Global image registry - prepended to all image repositories
  imageRegistry: "ghcr.io/glassflow/"
  
  # Observability configuration
  observability:
    metrics:
      enabled: true  # Enable metrics collection
    logs:
      enabled: false  # Enable log export
      exporter:
        otlp: {}  # OTLP exporter configuration
  
  # NATS global configuration
  nats:
    # NATS address for operator connection
    # Defaults to {{ .Release.Name }}-nats.{{ .Release.Namespace }}.svc.cluster.local
    address: ""
    stream:
      maxAge: 24h      # Maximum age of messages in streams
      maxBytes: 25GB   # Maximum size of streams
  
  # Pipeline namespace configuration
  pipelines:
    namespace:
      auto: true        # When true, operator creates per-pipeline namespaces (pipeline-<id>)
      name: "glassflow-pipelines"  # Fixed namespace to deploy all pipelines into (when auto is false)
      create: true      # When auto is false, Helm can optionally create the namespace
```

<Callout type="info">
**Pipeline Namespaces**:
- By default, the operator creates per-pipeline namespaces (`pipeline-<id>`)
- To use a fixed namespace for all pipelines, set `global.pipelines.namespace.auto: false`
- When `auto` is `false`, all pipelines deploy to the namespace specified in `global.pipelines.namespace.name`
</Callout>

### Key Global Settings

| Setting | Description | Default | Production Recommendation |
|---------|-------------|---------|---------------------------|
| `imageRegistry` | Global Docker registry prefix | `ghcr.io/glassflow/` | - |
| `observability.metrics.enabled` | Enable metrics collection | `true` | Keep enabled for monitoring |
| `observability.logs.enabled` | Enable log export | `false` | Enable for production monitoring |
| `observability.logs.exporter.otlp` | Your OTLP collector endpoint | `{}` | Configure your OTLP endpoint where glassflow will send logs. See [OTLP Exporter Configuration](/installation/kubernetes/observability#configure-otlp-exporter) for detailed setup |
| `nats.stream.maxAge` | Message retention period | `24h` | Adjust based on your data retention needs |
| `nats.stream.maxBytes` | Maximum stream size | `25GB` | Scale based on expected data volume |
| `pipelines.namespace.auto` | Create per-pipeline namespaces | `true` | Set to `false` to use fixed namespace |
| `pipelines.namespace.name` | Fixed namespace for all pipelines | `glassflow-pipelines` | Used when `auto` is `false` |
| `pipelines.namespace.create` | Create namespace if it doesn't exist | `true` | Only applies when `auto` is `false` |

## API Component

Configure the GlassFlow backend API service.

```yaml
api:
  # Scaling configuration
  replicas: 1
  logLevel: "INFO"
  
  # Container image settings
  image:
    repository: glassflow-etl-be
    tag: v2.4.0
    pullPolicy: IfNotPresent
  
  # Resource allocation
  resources:
    requests:
      memory: "100Mi"
      cpu: "100m"
    limits:
      memory: "200Mi"
      cpu: "250m"
  
  # Service configuration
  service:
    type: ClusterIP
    port: 8081
    targetPort: 8081
  
  # Environment variables
  env: []
```

### API Configuration Options

| Setting | Description | Default | Production Recommendation |
|---------|-------------|---------|---------------------------|
| `replicas` | Number of API instances | `1` | `1` is sufficient for API operations |
| `logLevel` | Logging verbosity | `INFO` | Use `DEBUG` for troubleshooting |
| `resources.requests` | Minimum resources | `100Mi/100m` | Scale based on load |
| `resources.limits` | Maximum resources | `200Mi/250m` | Set appropriate limits |


## UI Component

Configure the GlassFlow frontend user interface.

```yaml
ui:
  # Scaling configuration
  replicas: 1
  
  # Container image settings
  image:
    repository: glassflow-etl-fe
    tag: v2.4.0
    pullPolicy: IfNotPresent
  
  # Resource allocation
  resources:
    requests:
      memory: "512Mi"
      cpu: "100m"
    limits:
      memory: "1Gi"
      cpu: "200m"
  
  # Service configuration
  service:
    type: ClusterIP
    port: 8080
    targetPort: 8080
  
  # Environment variables (object format)
  env: {}
  
  # Kafka Kerberos Gateway sidecar (for connecting to Kerberos-secured Kafka clusters)
  kafkaGateway:
    enabled: true
    image:
      repository: kafka-kerberos-gateway
      tag: latest
      pullPolicy: IfNotPresent
    resources:
      requests:
        memory: "128Mi"
        cpu: "50m"
      limits:
        memory: "256Mi"
        cpu: "200m"
    port: 8082  # Internal port within the UI pod
```

### UI Configuration Options

| Setting | Description | Default | Production Recommendation |
|---------|-------------|---------|---------------------------|
| `replicas` | Number of UI instances | `1` | `1` is sufficient for UI pod |
| `resources.requests` | Minimum resources | `512Mi/100m` | Frontend typically needs more memory |
| `resources.limits` | Maximum resources | `1Gi/200m` | Adjust based on user load |
| `env` | Environment variables (object format) | `{}` | Use object format, not array |
| `kafkaGateway.enabled` | Enable Kafka Kerberos Gateway sidecar | `true` | Enable if connecting to Kerberos-secured Kafka |
| `kafkaGateway.resources` | Gateway resource requests/limits | `128Mi/50m` - `256Mi/200m` | Adjust based on usage |
| `kafkaGateway.port` | Gateway internal port | `8082` | Internal port within UI pod |

## GlassFlow Operator

Configure the Kubernetes operator that manages ETL pipelines resources in k8s. The operator chart and code is in a [separate repo](https://github.com/glassflow/glassflow-etl-k8s-operator)
and is deployed as a dependency chart.

```yaml
glassflow-operator:
  controllerManager:
    replicas: 1
    
    manager:
      # Operator image configuration
      image:
        repository: glassflow-etl-k8s-operator
        tag: v1.3.0
        pullPolicy: IfNotPresent
      
      # Resource allocation
      resources:
        requests:
          cpu: 10m
          memory: 64Mi
        limits:
          cpu: 500m
          memory: 128Mi
      
      # Service account configuration
      serviceAccount:
        annotations: {}
  
  # ETL component configurations
  glassflowComponents:
    ingestor:
      image:
        repository: glassflow-etl-ingestor
        tag: v2.4.0
      logLevel: "INFO"
      resources:
        requests:
          cpu: 1000m
          memory: 1Gi
        limits:
          cpu: 1500m
          memory: 1.5Gi
      affinity: {}
    
    join:
      image:
        repository: glassflow-etl-join
        tag: v2.4.0
      logLevel: "INFO"
      resources:
        requests:
          cpu: 1000m
          memory: 1Gi
        limits:
          cpu: 1500m
          memory: 1.5Gi
      affinity: {}
    
    sink:
      image:
        repository: glassflow-etl-sink
        tag: v2.4.0
      logLevel: "INFO"
      resources:
        requests:
          cpu: 1000m
          memory: 1Gi
        limits:
          cpu: 1500m
          memory: 1.5Gi
      affinity: {}
```

### Operator Configuration Options

| Component | CPU Request | Memory Request | CPU Limit | Memory Limit |
|-----------|-------------|----------------|-----------|--------------|
| Controller Manager | `10m` | `64Mi` | `500m` | `128Mi` |
| Ingestor | `1000m` | `1Gi` | `1500m` | `1.5Gi` |
| Join | `1000m` | `1Gi` | `1500m` | `1.5Gi` |
| Sink | `1000m` | `1Gi` | `1500m` | `1.5Gi` |

## NATS Configuration

NATS is the messaging system used for internal communication between GlassFlow components. 
Nats is deployed as a dependency chart using the [official nats charts repo](https://nats-io.github.io/k8s/helm/charts/)

```yaml
nats:
  # Enable/disable NATS deployment
  enabled: true
  
  # NATS container image
  container:
    image:
      repository: nats
      tag: 2.12.0-alpine
      pullPolicy: IfNotPresent
  
  # NATS configuration
  config:
    # Clustering for high availability
    cluster:
      enabled: true
      port: 6222
      replicas: 3  # Must be 2+ when JetStream is enabled
    
    # JetStream for persistent messaging
    jetstream:
      enabled: true
      
      # Memory store (fast, non-persistent)
      memoryStore:
        enabled: false
        maxSize: 1Gi
      
      # File store (persistent, recommended for production)
      fileStore:
        enabled: true
        dir: /data
        pvc:
          enabled: true
          size: 100Gi
          storageClassName: ""
    
    # Resource allocation
    resources:
      requests:
        memory: "2Gi"
        cpu: "500m"
      limits:
        memory: "4Gi"
        cpu: "1000m"
```

### NATS Configuration Options

| Setting | Description | Default | Production Recommendation |
|---------|-------------|---------|---------------------------|
| `enabled` | Deploy NATS with GlassFlow | `true` | Use external NATS for large deployments |
| `cluster.replicas` | Number of NATS nodes | `3` | Use 3+ for production |
| `jetstream.fileStore.pvc.size` | Storage size | `100Gi` | Scale based on data volume |
| `resources.requests` | Minimum resources | `2Gi/500m` | NATS is resource-intensive |

### NATS Prometheus Exporter

Nats Prometheus exporter collects all NATS related metrics. These metrics are provdied together with GlassFlow metrics on the `/metrics` endpoint. 
Details on accessing GlassFlow metrics can be found [here](/installation/kubernetes/observability#accessing-metrics)

```yaml
natsPrometheusExporter:
  image:
    repository: natsio/prometheus-nats-exporter
    tag: 0.17.3
    pullPolicy: IfNotPresent
  
  # Metrics to collect
  metrics:
    accstatz: true
    connz: true
    connz_detailed: true
    jsz: true
    gatewayz: true
    leafz: true
    routez: true
    subz: true
    varz: true
  
  service:
    type: ClusterIP
    port: 80
    targetPort: 7777
    protocol: TCP
    name: http
```

## Ingress Configuration

Configure external access to GlassFlow services.

```yaml
ingress:
  # Enable external access
  enabled: false
  
  # Ingress controller class
  ingressClassName: "nginx"  # or "traefik", "istio"
  
  # Ingress annotations
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
  
  # Host configurations
  hosts:
    - host: "glassflow.example.com"
      paths:
        - path: "/"
          pathType: Prefix
          serviceName: "glassflow-ui"
          servicePort: 8080
        - path: "/api/v1"
          pathType: Prefix
          serviceName: "glassflow-api"
          servicePort: 8081
  
  # TLS configuration
  tls:
    - hosts:
        - "glassflow.example.com"
      secretName: "glassflow-tls-secret"
```

### Ingress Configuration Options
By default, helm deployment does not expose GlassFlow to the internet. See [Using Ingress](/installation/kubernetes#using-ingress) for details on configuring ingress for enabling external access.

| Setting | Description | Default | Production Recommendation |
|---------|-------------|---------|---------------------------|
| `enabled` | Enable external access | `false` | Set to `true` for production |
| `ingressClassName` | Ingress controller | `""` | Specify your controller |
| `hosts` | Domain configurations | `[]` | Configure your domains |
| `tls` | HTTPS configuration | `[]` | Enable for production |

## Security Settings

Configure security contexts and service accounts.

```yaml
# Pod security context
podSecurityContext:
  fsGroup: 2000
  runAsNonRoot: true
  runAsUser: 1000

# Container security context
securityContext:
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 1000

# Service account configuration
serviceAccount:
  create: true
  automount: true
  annotations:
    eks.amazonaws.com/role-arn: "arn:aws:iam::ACCOUNT:role/ROLE"
  name: ""
```

### Security Configuration Options

| Setting | Description | Default | Production Recommendation |
|---------|-------------|---------|---------------------------|
| `podSecurityContext.fsGroup` | File system group | `{}` | Set for proper permissions |
| `securityContext.readOnlyRootFilesystem` | Read-only root filesystem | `{}` | Enable for security |
| `serviceAccount.create` | Create service account | `true` | Use existing for production |
| `serviceAccount.automount` | Automount API credentials | `true` | Enable for service account token mounting |
| `serviceAccount.name` | Service account name | `""` | Use custom name if needed |
| `serviceAccount.annotations` | Service account annotations | `{}` | Useful for IAM roles, OIDC providers |

## Pod Configuration

Configure pod-level settings for scheduling and labeling.

```yaml
# Pod annotations (useful for monitoring, logging, etc.)
podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "9090"
  prometheus.io/path: "/metrics"

# Pod labels
podLabels: {}

# Node selector for main pods (API and UI)
nodeSelector: {}

# Pod tolerations
tolerations: []

# Pod affinity rules
affinity: {}
```

### Pod Configuration Options

| Setting | Description | Default | Production Recommendation |
|---------|-------------|---------|---------------------------|
| `podAnnotations` | Additional pod annotations | `{}` | Add monitoring annotations |
| `podLabels` | Additional pod labels | `{}` | Useful for service discovery |
| `nodeSelector` | Node selector for scheduling | `{}` | Use for dedicated nodes |
| `tolerations` | Pod tolerations | `[]` | For tainted nodes |
| `affinity` | Pod affinity/anti-affinity rules | `{}` | Control pod placement |

## Autoscaling Configuration

Configure horizontal pod autoscaling for the API and UI components.

```yaml
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 5
  targetCPUUtilizationPercentage: 80
```

### Autoscaling Configuration Options

| Setting | Description | Default | Production Recommendation |
|---------|-------------|---------|---------------------------|
| `enabled` | Enable autoscaling | `false` | Enable for production workloads |
| `minReplicas` | Minimum number of replicas | `1` | Set based on minimum load |
| `maxReplicas` | Maximum number of replicas | `5` | Set based on peak load |
| `targetCPUUtilizationPercentage` | Target CPU utilization | `80` | Adjust based on workload |


## Best Practices

<Callout type="info">
**Production Checklist**:
- Use 3+ NATS replicas for high availability
- Set appropriate resource requests and limits
- Enable ingress with TLS
- Configure persistent storage for NATS
- Set up monitoring and logging
- Use node selectors for dedicated resources
</Callout>

### Resource Sizing Guidelines

| Environment | API CPU | API Memory | UI CPU | UI Memory | NATS CPU | NATS Memory | NATS Replicas | NATS Storage |
|-------------|---------|------------|--------|-----------|----------|-------------|---------------|--------------|
| Development | 50m | 50Mi | 50m | 256Mi | 200m | 1Gi | 1 | 10Gi |
| Production | 500m | 500Mi | 200m | 1Gi | 1000m | 4Gi | 3 | 100Gi |
| High-Performance | 1000m | 1Gi | 500m | 2Gi | 2000m | 8Gi | 5 | 500Gi |

### Monitoring Configuration

```yaml
# Enable comprehensive monitoring
global:
  observability:
    metrics:
      enabled: true
    logs:
      enabled: true
      exporter:
        otlp:
          endpoint: "https://your-otel-collector:4317"
          tls:
            insecure: false

# Add monitoring annotations
podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "9090"
  prometheus.io/path: "/metrics"
```

## Troubleshooting

### Common Issues

1. **NATS Connection Issues**
   ```yaml
   # Ensure NATS is properly configured
   nats:
     config:
       cluster:
         replicas: 3  # Must be 2+ for JetStream
   ```

2. **Resource Constraints**
   ```yaml
   # Check resource requests vs limits
   resources:
     requests:
       memory: "100Mi"  # Should be realistic
       cpu: "100m"
     limits:
       memory: "200Mi"  # Should be higher than requests
       cpu: "250m"
   ```

3. **Ingress Not Working**
   ```yaml
   # Verify ingress configuration
   ingress:
     enabled: true
     ingressClassName: "nginx"  # Must match your controller
     hosts:
       - host: "your-domain.com"
   ```

### Validation Commands

```bash
# Validate Helm values
helm template glassflow glassflow/glassflow-etl -f values.yaml --dry-run

# Check resource usage
kubectl top pods -n glassflow

# Verify services
kubectl get svc -n glassflow

# Check ingress
kubectl get ingress -n glassflow
```

## Next Steps

After configuring your `values.yaml`:

1. **Install GlassFlow**: `helm install glassflow glassflow/glassflow-etl -f values.yaml`
2. **Verify Installation**: Check pod status and service endpoints
3. **Configure Monitoring**: Set up Prometheus/Grafana dashboards
4. **Set Up Logging**: Configure log aggregation
5. **Test Functionality**: Create your first ETL pipeline

For more information, see the [Installation Guide](/installation/kubernetes) and [Pipeline JSON Reference](/configuration/pipeline-json-reference).
