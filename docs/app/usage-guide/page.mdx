---
title: 'Usage Guide'
asIndexPage: true
description: 'Learn how to create and manage data pipelines with GlassFlow'
---

# Usage Guide

This guide will walk you through the process of creating and managing data pipelines using GlassFlow. We'll cover everything from initial setup to monitoring your pipeline's performance.

## Prerequisites

Before creating your pipeline, ensure you have:

1. GlassFlow running locally (see [Installation Guide](/installation))
2. Access to your Kafka cluster
3. Access to your ClickHouse database
4. The following information ready:
   - Kafka connection details
   - ClickHouse connection details
   - Source topic names
   - Target table names

## Creating a Pipeline

GlassFlow provides two ways to create a pipeline, each suited for different use cases and preferences:

### 1. Web UI (Recommended for Beginners)

The **Web UI** approach uses a visual wizard that guides you through each step of pipeline creation. This method is ideal for:

- Creating your first pipeline and understanding the components
- Visual learners who prefer a guided interface
- Quick prototyping and experimentation

The wizard walks you through:
- Configuring Kafka connections and topic selection
- Setting up transformations (deduplication, joins, filters)
- Configuring ClickHouse connections and table mapping
- Field mapping and data type configuration

[Learn how to use the Web UI →](/usage-guide/web-ui)

### 2. Python SDK (Recommended for Advanced Users)

The **Python SDK** approach allows for programmatic creation of pipelines. This method is ideal for:

- Developers who prefer code-based configuration
- Automated pipeline deployment and CI/CD integration
- Complex pipeline configurations that benefit from version control
- Integration with existing Python-based data workflows

The SDK provides:
- Type-safe pipeline configuration
- Integration with Python data processing libraries
- Version control and code review capabilities
- Automated testing and validation

[Learn how to use the Python SDK →](/usage-guide/python-sdk)

Both approaches create the same underlying pipeline configuration and can be used interchangeably based on your workflow preferences.

## Verifying Data Flow

1. **Check Kafka Topics**
   - Verify data is being produced
   - Check message format
   - Monitor topic health

2. **Monitor ClickHouse**
   - Verify data arrival
   - Check data quality
   - Monitor table growth

3. **Monitor the Pipeline logs**

Pipeline logs are available via kubectl. To monitor your GlassFlow deployment:

```bash
# View all GlassFlow pods
kubectl get pods -n glassflow

# Follow logs in real-time for the API
kubectl logs -n glassflow deployment/glassflow-api -f

# Follow logs in real-time for the UI
kubectl logs -n glassflow deployment/glassflow-ui -f
```